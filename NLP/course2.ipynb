{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37303464",
   "metadata": {},
   "source": [
    "Imports the spaCy library and downloads the medium English model (en_core_web_md), which includes word vectors for semantic similarity tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6fc0a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "     ---------------------------------------- 0.0/33.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.3/33.5 MB ? eta -:--:--\n",
      "     - -------------------------------------- 1.0/33.5 MB 3.0 MB/s eta 0:00:11\n",
      "     - -------------------------------------- 1.6/33.5 MB 3.4 MB/s eta 0:00:10\n",
      "     --- ------------------------------------ 2.6/33.5 MB 3.4 MB/s eta 0:00:09\n",
      "     ---- ----------------------------------- 3.4/33.5 MB 3.5 MB/s eta 0:00:09\n",
      "     ---- ----------------------------------- 3.9/33.5 MB 3.5 MB/s eta 0:00:09\n",
      "     ----- ---------------------------------- 5.0/33.5 MB 3.6 MB/s eta 0:00:09\n",
      "     ------ --------------------------------- 5.8/33.5 MB 3.6 MB/s eta 0:00:08\n",
      "     ------- -------------------------------- 6.6/33.5 MB 3.7 MB/s eta 0:00:08\n",
      "     -------- ------------------------------- 7.3/33.5 MB 3.7 MB/s eta 0:00:08\n",
      "     --------- ------------------------------ 8.1/33.5 MB 3.7 MB/s eta 0:00:07\n",
      "     ---------- ----------------------------- 9.2/33.5 MB 3.7 MB/s eta 0:00:07\n",
      "     ------------ --------------------------- 10.2/33.5 MB 3.8 MB/s eta 0:00:07\n",
      "     ------------- -------------------------- 11.0/33.5 MB 3.8 MB/s eta 0:00:06\n",
      "     -------------- ------------------------- 12.1/33.5 MB 3.9 MB/s eta 0:00:06\n",
      "     --------------- ------------------------ 13.1/33.5 MB 4.0 MB/s eta 0:00:06\n",
      "     ---------------- ----------------------- 14.2/33.5 MB 4.1 MB/s eta 0:00:05\n",
      "     ------------------ --------------------- 15.2/33.5 MB 4.1 MB/s eta 0:00:05\n",
      "     ------------------- -------------------- 16.3/33.5 MB 4.2 MB/s eta 0:00:05\n",
      "     -------------------- ------------------- 17.6/33.5 MB 4.2 MB/s eta 0:00:04\n",
      "     ---------------------- ----------------- 18.6/33.5 MB 4.3 MB/s eta 0:00:04\n",
      "     ----------------------- ---------------- 19.7/33.5 MB 4.3 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 21.0/33.5 MB 4.4 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 21.8/33.5 MB 4.4 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 23.1/33.5 MB 4.5 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 24.1/33.5 MB 4.5 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 25.4/33.5 MB 4.5 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 26.5/33.5 MB 4.6 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 27.5/33.5 MB 4.6 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 28.8/33.5 MB 4.6 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 29.9/33.5 MB 4.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 30.9/33.5 MB 4.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 32.0/33.5 MB 4.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 32.5/33.5 MB 4.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  33.0/33.5 MB 4.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  33.3/33.5 MB 4.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 33.5/33.5 MB 4.4 MB/s  0:00:07\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0a19d8",
   "metadata": {},
   "source": [
    "Loads the medium English model and reads text from wiki_us.txt.\n",
    "Processes it with spaCy’s NLP pipeline, storing the result in doc.\n",
    "Extracts the first sentence from the document as sentence1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fad3540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "with open (\"data/wiki_us.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "doc = nlp(text)\n",
    "sentence1 = list(doc.sents)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ac8590a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States of America (U.S.A. or USA), commonly known as the United States (U.S. or US) or America, is a country primarily located in North America.\n"
     ]
    }
   ],
   "source": [
    "print(sentence1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5567b70e",
   "metadata": {},
   "source": [
    "Finds the 10 most similar words to \"country\" using spaCy’s word vectors.\n",
    "It retrieves semantically related terms based on vector similarity, showing how words with similar meanings are close in vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81f54d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SLUMS', 'inner-city', 'anti-poverty', 'Socioeconomic', 'INTERSECT', 'Divides', 'dropout', 'handicaps', 'drop-out', 'south-east']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "your_word = \"country\"\n",
    "\n",
    "ms = nlp.vocab.vectors.most_similar(\n",
    "    np.asarray([nlp.vocab.vectors[nlp.vocab.strings[your_word]]]), n=10)\n",
    "words = [nlp.vocab.strings[w] for w in ms[0][0]]\n",
    "distances = ms[2]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d411a592",
   "metadata": {},
   "source": [
    "Creates two short documents and computes their semantic similarity using word vectors.\n",
    "The .similarity() method returns a numerical score (0–1), where higher values mean stronger semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f216a265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like salty fries and hamburgers. <-> Fast food tastes very good. 0.8015960454940796\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"I like salty fries and hamburgers.\")\n",
    "doc2 = nlp(\"Fast food tastes very good.\")\n",
    "\n",
    "# Similarity of two documents\n",
    "print(doc1, \"<->\", doc2, doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ff4c1c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like pizza <-> I like cars 0.7633953094482422\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(\"I like pizza\")\n",
    "doc4 = nlp(\"I like cars\")\n",
    "\n",
    "# Similarity of two documents\n",
    "print(doc3, \"<->\", doc4, doc3.similarity(doc4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a35fa3d",
   "metadata": {},
   "source": [
    "Creates a blank English pipeline with only a sentencizer component — this splits text into sentences without doing token tagging, parsing, or NER.\n",
    "It’s useful for lightweight tasks or custom pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aa8ab60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x1d1ea4b3550>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e3d048",
   "metadata": {},
   "source": [
    "Displays the current components in the pipeline and their statuses (trainable, frozen, etc.).\n",
    "Here, it will show just the sentencizer since this is a blank model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dfeeccf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'sentencizer': {'assigns': ['token.is_sent_start', 'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['sents_f', 'sents_p', 'sents_r'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'sentencizer': []},\n",
       " 'attrs': {'doc.sents': {'assigns': ['sentencizer'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['sentencizer'], 'requires': []}}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ec8881",
   "metadata": {},
   "source": [
    "Loads the small English model (en_core_web_sm) — which includes tagging, parsing, and NER — and analyzes its full pipeline components.\n",
    "This helps you compare how the small model differs from the blank one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "86191f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "35bdc201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc',\n",
       "    'pos_acc',\n",
       "    'tag_micro_p',\n",
       "    'tag_micro_r',\n",
       "    'tag_micro_f'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': []},\n",
       " 'attrs': {'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []}}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp2.analyze_pipes()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
